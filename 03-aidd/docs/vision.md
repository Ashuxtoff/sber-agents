# Техническое видение проекта

## Обзор
Простой LLM-ассистент в виде Telegram-бота для проверки гипотезы.

## Технологии

### Основной стек
- **Python 3.11+** - язык разработки
- **uv** - менеджер зависимостей и виртуальных окружений
- **aiogram 3.x** - фреймворк для работы с Telegram Bot API (polling режим)
- **openai** - клиент для работы с LLM через OpenRouter

### Инструменты разработки
- **make** - автоматизация сборки и запуска проекта
- **.env** - конфигурация через переменные окружения

### Принцип выбора
Минимальный набор проверенных инструментов. Никаких дополнительных ORM, очередей, БД на старте.

## Принцип разработки

### Подход KISS / YAGNI
- Один файл `bot.py` с простой структурой
- Без репозиториев, сервисов, слоев абстракции
- Код разделен на секции через комментарии
- Логирование через `print()` (стандартный вывод)
- Никаких БД, кэшей, сложных патернов

### Итеративность
1. Минимальный работающий прототип
2. Тестирование гипотезы на реальных пользователях
3. Добавление фич только при необходимости

## Структура проекта

```
03-aidd/
├── bot.py          # весь код бота
├── .env            # конфигурация (не коммитим)
├── .gitignore      # игнорируем .env, __pycache__, .venv
├── pyproject.toml  # зависимости через uv
├── Makefile        # команды для запуска
├── README.md       # как запустить
└── docs/           # документация
    ├── idea.md
    └── vision.md
```

**Принцип**: Минимум файлов. Вся логика в одном месте.

## Архитектура проекта

### Структура bot.py
```python
# 1. Импорты
# 2. Конфигурация (загрузка .env)
# 3. Инициализация (bot, openai client)
# 4. Хендлеры (обработка сообщений)
# 5. Запуск (polling)
```

### Флоу обработки
1. Пользователь отправляет сообщение в Telegram
2. Хендлер получает сообщение
3. Формируется запрос к LLM (системный промпт + сообщение)
4. LLM через OpenRouter возвращает ответ
5. Ответ отправляется пользователю

### Упрощения
- **Нет состояний** - каждая сессия независима
- **Нет истории** - каждый запрос обрабатывается отдельно
- **Нет очередей** - последовательная обработка
- **Прямой вызов LLM** - без middleware, прокси, кэшей

## Модель данных

### Сущности
- **Сообщение пользователя** - текст из Telegram
- **Системный промпт** - статическая строка с ролью ассистента
- **Ответ LLM** - текст от модели

### Данные
Нет персистентного хранилища. Все в памяти:
- Сообщения обрабатываются и забываются
- Нет истории диалога
- Нет пользовательских настроек
- Нет метаданных

### Структуры
Простые типы Python:
- `str` - текст сообщений
- `dict` - конфигурация из .env

## Работа с LLM

### Провайдер
**OpenRouter** - агрегатор LLM API

### API
- Клиент: `openai.OpenAI()` с кастомным `base_url`
- Эндпоинт: `https://openrouter.ai/api/v1`
- Аутентификация: `Authorization: Bearer {OPENROUTER_API_KEY}`

### Промпты
- **Системный промпт** - статическая строка в коде (роль ассистента)
- **Пользовательский запрос** - текст сообщения из Telegram

### Параметры
Минимум необходимых параметров:
- `model` - название модели (из .env)
- `messages` - список сообщений (system + user)
- `temperature` - опционально (по умолчанию 0.7)

### Обработка
- Прямой вызов через `openai.ChatCompletion.create()`
- Ответ парсим и отправляем пользователю
- Ошибки логируются через `print()`

## Сценарии работы

### Сценарий 1: Успешный ответ
1. Пользователь пишет сообщение в бот
2. Бот показывает "typing..." (optional)
3. Формируется запрос к LLM
4. Получен ответ от LLM
5. Бот отправляет ответ пользователю

### Сценарий 2: Ошибка
1. Пользователь пишет сообщение в бот
2. Ошибка при вызове LLM (таймаут, неверный ключ, etc)
3. Бот отправляет сообщение об ошибке пользователю
4. Ошибка логируется через `print()`

### Упрощения
- **Нет retry** - первая ошибка == пользователь видит ошибку
- **Нет fallback** - один источник LLM
- **Нет очередей** - обработка в реальном времени

## Подход к конфигурированию

### Файл .env
```env
# Telegram
TELEGRAM_BOT_TOKEN=your_bot_token

# OpenRouter
OPENROUTER_API_KEY=your_api_key
OPENROUTER_MODEL=meta-llama/llama-3.1-8b-instruct:free
```

### Загрузка конфигурации
- Библиотека: `python-dotenv` (опционально, можно `os.getenv`)
- Файл `.env` в корне проекта
- Чтение при старте бота
- Нет валидации - валидность проверяется при использовании

### Переменные
Минимальный набор:
- `TELEGRAM_BOT_TOKEN` - токен бота от @BotFather
- `OPENROUTER_API_KEY` - ключ для OpenRouter API
- `OPENROUTER_MODEL` - модель для использования

### Упрощения
- Нет иерархии конфигов (dev/prod)
- Нет дефолтных значений в коде
- Нет конфигурационных файлов в YAML/JSON/TOML

## Подход к логгированию

### Инструмент
**`print()`** - стандартный вывод Python

### Что логируем
- Старт/остановка бота
- Ошибки при вызове LLM
- Ошибки Telegram API
- Критические ошибки (exception с traceback)

### Формат
Простые строки с человекочитаемой информацией:
```python
print(f"[START] Bot started")
print(f"[ERROR] Failed to call LLM: {error}")
```

### Упрощения
- **Нет уровней логирования** (DEBUG/INFO/WARN/ERROR)
- **Нет форматирования** (JSON, структурированные логи)
- **Нет ротации логов** - вывод только в stdout
- **Нет фильтрации** - все события равнозначны
- **Нет внешних библиотек** - только встроенный `print()`

### Направление вывода
- `stdout` - все логи в стандартный вывод
- Можно перенаправить в файл: `python bot.py > bot.log 2>&1`
- Можно использовать `tee` для мониторинга
